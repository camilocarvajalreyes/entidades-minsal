{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo de modelo BERT para detección de entidades\n",
    "\n",
    "En este notebook:\n",
    "- Mostramos como cargar nuestros modelos usando HuggingFace\n",
    "- Transformamos su output a etiquetas de entidades interpretables\n",
    "\n",
    "Su objetivo es ejecutar los algoritmos desde cero teniendo sólamente un ambiente conda. Su público objetivo es alguien que se interese en como los modelos tratan los datos. Si el objetivo es únicamente probar el modelo en un input de manera directa ver el notebook **demo_minimalista.ipynb** (que sin embargo requiere instalación previa).\n",
    "\n",
    "Fue probado en un notebook Linux con ambiente conda instalado, versión de python 3.9.0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalación de bibliotecas necesarias\n",
    "!pip install numpy==1.20.0\n",
    "!pip install pandas==1.4.3\n",
    "!pip install scipy==1.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch\n",
    "!pip install transformers==4.23.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "MODEL = 'ccarvajal/beto-prescripciones-medicas'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=11, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ambos tokenizer y modelos, se puede guardar una copia local usando el método .save_pretrained()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output del modelo\n",
    "\n",
    "Definimos dos funciones auxiliares que nos ayudarán con el etiquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "def word_ids_method(text,tokenizer):\n",
    "    \"\"\"Método que asigna el primer token (subword) de una palabra como representante\n",
    "        La etiqueta de este token será la etiqueta de la palabra\n",
    "    Método integrado en tokenize_and_align_labels\n",
    "    Fuente: https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "\n",
    "    Argumentos\n",
    "\n",
    "        text: str o List[str]\n",
    "            texto a tokenizar, si \n",
    "    \"\"\"\n",
    "    if not isinstance(text,list):\n",
    "        text = text.split()\n",
    "    n_words = len(text)\n",
    "\n",
    "    tokenized_inputs = tokenizer([text], truncation=True, is_split_into_words=True)\n",
    "    mask = []\n",
    "    # for i in range(n_words):\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            mask.append(0)\n",
    "        elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def eval_text(text,tokenizer,model):\n",
    "    \"\"\"\n",
    "    Toma un texto (lista de palabras o string), un tokenizador y modelo de HuggingFace\n",
    "    Retorna el output del modelo (ids de entidades)\n",
    "    \"\"\"\n",
    "    mask = word_ids_method(text,tokenizer)\n",
    "    encoded_input = tokenizer(text,return_tensors='pt',is_split_into_words=isinstance(text,list))\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    result = np.argmax(scores,axis=1)\n",
    "\n",
    "    return result[mask==np.array(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\"\n",
    "\n",
    "print(text)\n",
    "eval_text(text,tokenizer,model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos ya un output de nuestro modelo con cinco entidades, sin embargo nos gustaría tener las entidades con sus respectivas etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities(y_pred,map_dict,return_type='list'):\n",
    "    inv_map = {v: k for k, v in map_dict.items()}\n",
    "    if return_type == 'list':\n",
    "        return [inv_map[y] for y in y_pred]\n",
    "    else:\n",
    "        return np.array([inv_map[y] for y in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades = map_entities(eval_text(text,tokenizer,model),ner_dict)\n",
    "for i, ent in enumerate(entidades):\n",
    "    print(\"{}\\t{}\".format(text.split()[i],ent))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modelo para etiquetas ADMIN más finas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_admin = 'ccarvajal/beto-prescripciones-medicas-ADMIN'\n",
    "\n",
    "tokenizer_admin = AutoTokenizer.from_pretrained(MODEL_admin)\n",
    "model_admin = AutoModelForTokenClassification.from_pretrained(MODEL_admin, num_labels=7, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_ner_dict = {\n",
    "    'O': 0,\n",
    "    'B-CANT': 1,\n",
    "    'I-CANT': 2,\n",
    "    'B-UND':3,\n",
    "    'I-UND':4,\n",
    "    'B-VIA_ADMIN': 5,\n",
    "    'I-VIA_ADMIN': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_admin(entidades:list,texto:str) -> str:\n",
    "    \"\"\"Retorna un substring correspondiente a aquellas entidades etiquetadas con admin y los indices donde esto ocurre\"\"\"\n",
    "    indices = [i for i, ent in enumerate(entidades) if ent == 'B-ADMIN' or ent == 'I-ADMIN']\n",
    "    return ' '.join([token for i, token in enumerate(texto.split(' ')) if i in indices]), indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etiquetar(texto):\n",
    "    entidades = map_entities(eval_text(texto,tokenizer,model),ner_dict)\n",
    "    admin_text, indices = get_admin(entidades,texto)\n",
    "    entidades_admin = map_entities(eval_text(admin_text,tokenizer_admin,model_admin),admin_ner_dict)\n",
    "    for i, ent_admin in enumerate(entidades_admin):\n",
    "        entidades[indices[i]] = ent_admin\n",
    "    return entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entidades = etiquetar(text)\n",
    "for i, ent in enumerate(entidades):\n",
    "    print(\"{}\\t{}\".format(text.split()[i],ent))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente mostramos una manera más bonita de mostrar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def render_pandas(ents,text_list):\n",
    "    data = {'ACTIVE_PRINCIPLE':'','FORMA_FARMA':'','CANT-ADMIN':'','UND-ADMIN':'','VIA-ADMIN':'','PERIODICITY':'','DURATION':''}\n",
    "\n",
    "    for i, ent in enumerate(ents):\n",
    "        if '-ACTIVE_PRINCIPLE' in ent:\n",
    "            data['ACTIVE_PRINCIPLE'] += ' ' + text_list[i]\n",
    "        elif '-FORMA_FARMA' in ent:\n",
    "            data['FORMA_FARMA'] += ' ' + text_list[i]\n",
    "        elif '-CANT' in ent:\n",
    "            data['CANT-ADMIN'] += ' ' + text_list[i]\n",
    "        elif '-UND' in ent:\n",
    "            data['UND-ADMIN'] += ' ' + text_list[i]\n",
    "        elif '-VIA_ADMIN' in ent:\n",
    "            data['VIA-ADMIN'] += ' ' + text_list[i]\n",
    "        elif '-PERIODICITY' in ent:\n",
    "            data['PERIODICITY'] += ' ' + text_list[i]\n",
    "        elif '-DURATION' in ent:\n",
    "            data['DURATION'] += ' ' + text_list[i]\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "    return df.style.set_table_styles([dict(selector='th', props=[('text-align', 'center')])]).hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_pandas(entidades,text.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (default, Nov 15 2020, 14:28:56) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a09f83d747ddbee076343c75140ab6a5612c6071f1d1b8cd9bf117d48cfe15b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
