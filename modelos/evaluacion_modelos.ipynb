{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"bert-clinical-scratch-wl-es-NER-prescription\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxfunctions import eval_text, map_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0,  0,  3,  0,  0,  5,  7,  8,  8,  9, 10, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\"\n",
    "# text = text.split()\n",
    "\n",
    "eval_text(text,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['B-ACTIVE_PRINCIPLE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FORMA_FARMA',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ADMIN',\n",
       " 'B-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'B-DURATION',\n",
       " 'I-DURATION',\n",
       " 'I-DURATION']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }\n",
    "\n",
    "print(text)\n",
    "map_entities(eval_text(text,tokenizer,model),ner_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjuntos test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../datos/procesamiento')\n",
    "from corpus import Corpus\n",
    "\n",
    "datos_conll = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_conll.entidades = ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 250 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "1003\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    datos_conll.load_conll('../datos/Etiquetado/corpus_s{}_etiquetados.conll'.format(i+1))\n",
    "\n",
    "print(len(datos_conll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 802\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 201\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_data_mini = datos_conll.to_HF_dataset()\n",
    "\n",
    "HF_dataset_mini = HF_data_mini.train_test_split(test_size=0.2,seed=0)\n",
    "HF_dataset_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc0e799fc0648618b50d8fa40fea0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ff1ac5a67b49899e21d54e70771d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from auxfunctions import tokenize_and_align_labels\n",
    "process = lambda examples: tokenize_and_align_labels(examples,tokenizer)\n",
    "\n",
    "tokenized_data_mini = HF_dataset_mini.map(process, batched=True)\n",
    "tokenized_data_mini = tokenized_data_mini.remove_columns(['id','tokens','ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluación\n",
      "\t f1: 0.50 | precision: 0.53 | recall: 0.47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5299600532623169, 0.4654970760233918, 0.49564134495641343)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auxfunctions import eval_text, map_entities, calculate_metrics\n",
    "\n",
    "y_test = [row['ner_tags'] for row in HF_dataset_mini['test']]\n",
    "y_preds = [list(eval_text(row['tokens'],tokenizer,model)) for row in HF_dataset_mini['test']]\n",
    "\n",
    "calculate_metrics(y_preds,y_test,ner_dict=ner_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test basado en ER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 51183 secuencias de token-entidad al corpus\n",
      "154552\n"
     ]
    }
   ],
   "source": [
    "corpus_test = Corpus()\n",
    "corpus_test.entidades = ner_dict\n",
    "\n",
    "corpus_test.load_conll('../datos/Etiquetado/corpus_ER_test.txt')\n",
    "print(len(corpus_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_test = corpus_test.to_HF_dataset()\n",
    "HF_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('entidades_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60cab8a6eccdbb291cd8546425070b6bbfbad3cae25f59923f3919ec9d23e6db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
