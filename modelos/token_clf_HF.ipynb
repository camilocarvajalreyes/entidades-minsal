{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de tokens con HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../datos/procesamiento')\n",
    "from corpus import Corpus\n",
    "\n",
    "corpus_train = Corpus()\n",
    "corpus_test = Corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cargando el corpus como dataset de HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }\n",
    "\n",
    "corpus_train.entidades = ner_dict\n",
    "corpus_test.entidades = ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 82481 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_train.load_conll('../datos/Etiquetado/corpus_ER_train_v2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 20621 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_test.load_conll('../datos/Etiquetado/corpus_ER_test_v2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "HF_dataset = DatasetDict()\n",
    "\n",
    "HF_dataset['train'] = corpus_train.to_HF_dataset()\n",
    "HF_dataset['test'] = corpus_test.to_HF_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 82481\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20621\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos un modelo basado en Transformers de HuggingFace\n",
    "\n",
    "Nuestro modelo será: plncmm/bert-clinical-scratch-wl-es. Ha sido fine-tuneado con texto médico (aunque probablemente no con prescripciones)\n",
    "\n",
    "[Fuente de esta sección](https://huggingface.co/docs/transformers/tasks/token_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"plncmm/bert-clinical-scratch-wl-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el tokenizador para codificar nuestro input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'ácido',\n",
       " 'val',\n",
       " '##pro',\n",
       " '##ico',\n",
       " '250',\n",
       " 'mg',\n",
       " 'comprimido',\n",
       " 'recu',\n",
       " '##bier',\n",
       " '##to',\n",
       " '1',\n",
       " 'comprimido',\n",
       " 'oral',\n",
       " 'diaria',\n",
       " '0',\n",
       " '-',\n",
       " '0',\n",
       " '-',\n",
       " '250',\n",
       " '##m',\n",
       " '##g',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = HF_dataset[\"train\"][0]\n",
    "\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos corregir el _mismatch_ entre input tokenizado y la lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from auxfunctions import tokenize_and_align_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48f89ffd8af495e94c5fb8c827c1add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ab4d76657b4bc3816e9d4581a7ce94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = HF_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_data = tokenized_data.remove_columns(['id','tokens','ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': InMemoryTable\n",
       " input_ids: list<item: int32>\n",
       "   child 0, item: int32\n",
       " token_type_ids: list<item: int8>\n",
       "   child 0, item: int8\n",
       " attention_mask: list<item: int8>\n",
       "   child 0, item: int8\n",
       " labels: list<item: int64>\n",
       "   child 0, item: int64\n",
       " ----\n",
       " input_ids: [[[4,11044,1590,4769,1248,...,1139,9392,30967,30972,5],[4,27576,3356,1087,30968,...,29258,1751,5631,15359,5],...,[4,23565,1820,27604,5880,...,1748,1129,1003,2596,5],[4,4017,5026,7736,1316,...,5631,1748,997,2596,5]],[[4,15772,13955,3284,16056,...,2596,1672,2286,12873,5],[4,13695,1820,1204,1791,...,1748,1129,1003,2596,5],...,[4,20482,6180,30957,4665,...,2596,1672,2286,12873,5],[4,30305,7409,1108,19333,...,1411,2242,1074,1707,5]],...,[[4,1097,23070,14921,4665,...,1139,1444,1139,1129,5],[4,1651,4714,3886,14008,...,2596,1672,1002,12873,5],...,[4,1097,23070,14921,3284,...,1013,989,1001,30974,5],[4,19769,2470,19624,16057,...,2596,1672,1129,12873,5]],[[4,24236,21335,1575,1444,...,15359,1672,1098,12873,5],[4,14053,4021,18229,1405,...,1098,29258,12791,15359,5],...,[4,1097,23070,14921,4665,...,12791,1748,1001,2596,5],[4,5]]]\n",
       " token_type_ids: [[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],...,[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0]]]\n",
       " attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],...,[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1]]]\n",
       " labels: [[[-100,1,2,-100,-100,...,-100,-100,-100,-100,-100],[-100,1,-100,-100,-100,...,0,0,0,7,-100],...,[-100,1,-100,-100,-100,...,7,8,-100,8,-100],[-100,1,-100,-100,-100,...,0,7,8,8,-100]],[[-100,0,-100,0,0,...,8,9,10,10,-100],[-100,1,-100,-100,-100,...,7,8,-100,8,-100],...,[-100,1,-100,-100,2,...,8,9,10,10,-100],[-100,0,-100,-100,-100,...,7,8,8,8,-100]],...,[[-100,1,-100,-100,2,...,-100,-100,-100,-100,-100],[-100,1,-100,-100,-100,...,8,9,10,10,-100],...,[-100,0,-100,-100,0,...,0,-100,-100,-100,-100],[-100,1,-100,-100,-100,...,8,9,10,10,-100]],[[-100,1,2,-100,2,...,7,9,10,10,-100],[-100,1,-100,-100,-100,...,0,0,5,7,-100],...,[-100,1,-100,-100,2,...,5,7,8,8,-100],[-100,-100]]],\n",
       " 'test': InMemoryTable\n",
       " input_ids: list<item: int32>\n",
       "   child 0, item: int32\n",
       " token_type_ids: list<item: int8>\n",
       "   child 0, item: int8\n",
       " attention_mask: list<item: int8>\n",
       "   child 0, item: int8\n",
       " labels: list<item: int64>\n",
       "   child 0, item: int64\n",
       " ----\n",
       " input_ids: [[[4,5121,1847,1791,3478,...,2596,1672,2286,12873,5],[4,18956,13908,2287,1062,...,12791,1748,997,2596,5],...,[4,6265,1446,1600,30962,...,30962,1748,1992,2596,5],[4,1383,13141,1184,1316,...,1707,1672,999,12873,5]],[[4,25614,1405,11761,20035,...,2596,1672,1129,2817,5],[4,26122,1343,7736,1316,...,1976,1748,1001,2596,5],...,[4,1097,23070,14921,4665,...,2596,1672,1413,12873,5],[4,18956,13908,2287,1062,...,1008,1413,30967,30972,5]],...,[[4,5647,1699,23565,15709,...,12791,1748,1992,2596,5],[4,1170,2455,1405,3284,...,1139,1444,1139,1444,5],...,[4,5183,3356,24405,30956,...,15080,7725,9934,2940,5],[4,1710,28111,1083,1098,...,1748,1129,1003,2596,5]],[[4,15772,13955,3284,16056,...,1707,1672,999,12873,5],[4,11236,14059,1680,1405,...,2155,1748,1001,2596,5],...,[4,18104,1872,14027,1316,...,15126,2155,9934,2940,5],[4,5]]]\n",
       " token_type_ids: [[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],...,[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0]]]\n",
       " attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],...,[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1]]]\n",
       " labels: [[[-100,1,-100,-100,2,...,8,9,10,10,-100],[-100,1,-100,-100,-100,...,5,7,8,8,-100],...,[-100,1,-100,-100,-100,...,-100,7,8,8,-100],[-100,1,-100,-100,-100,...,8,9,10,10,-100]],[[-100,1,-100,2,-100,...,8,9,10,10,-100],[-100,1,-100,-100,-100,...,-100,7,8,8,-100],...,[-100,1,-100,-100,2,...,8,9,10,10,-100],[-100,1,-100,-100,-100,...,-100,-100,-100,-100,-100]],...,[[-100,1,-100,2,-100,...,5,7,8,8,-100],[-100,1,-100,-100,2,...,-100,-100,-100,-100,-100],...,[-100,0,-100,-100,-100,...,0,-100,0,0,-100],[-100,1,-100,-100,2,...,7,8,-100,8,-100]],[[-100,0,-100,0,0,...,8,9,10,10,-100],[-100,1,-100,-100,-100,...,-100,7,8,8,-100],...,[-100,0,-100,-100,-100,...,-100,-100,0,0,-100],[-100,-100]]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at plncmm/bert-clinical-scratch-wl-es were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at plncmm/bert-clinical-scratch-wl-es and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=len(ner_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset = tokenized_data[\"train\"],\n",
    "    eval_dataset = tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 82481\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25780\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea97171a37c44e739cf164c5aeeefb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3016, 'learning_rate': 1.961210240496509e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0723, 'learning_rate': 1.922420480993018e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0391, 'learning_rate': 1.8836307214895268e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0253, 'learning_rate': 1.844840961986036e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0194, 'learning_rate': 1.806051202482545e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0158, 'learning_rate': 1.7672614429790537e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0106, 'learning_rate': 1.7284716834755626e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0096, 'learning_rate': 1.6896819239720715e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0096, 'learning_rate': 1.6508921644685803e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0085, 'learning_rate': 1.6121024049650892e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20621\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ae3473b899420081d5709bc4440df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.004957306664437056, 'eval_runtime': 66.1481, 'eval_samples_per_second': 311.74, 'eval_steps_per_second': 19.487, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0054, 'learning_rate': 1.5733126454615984e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0039, 'learning_rate': 1.5345228859581073e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-6500\n",
      "Configuration saved in ./results/checkpoint-6500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0059, 'learning_rate': 1.4957331264546162e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-7000\n",
      "Configuration saved in ./results/checkpoint-7000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'learning_rate': 1.456943366951125e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-7500\n",
      "Configuration saved in ./results/checkpoint-7500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0056, 'learning_rate': 1.4181536074476339e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-8000\n",
      "Configuration saved in ./results/checkpoint-8000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'learning_rate': 1.379363847944143e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-8500\n",
      "Configuration saved in ./results/checkpoint-8500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.004, 'learning_rate': 1.3405740884406518e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-9000\n",
      "Configuration saved in ./results/checkpoint-9000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0037, 'learning_rate': 1.3017843289371609e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-9500\n",
      "Configuration saved in ./results/checkpoint-9500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'learning_rate': 1.2629945694336696e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-10000\n",
      "Configuration saved in ./results/checkpoint-10000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0039, 'learning_rate': 1.2242048099301784e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20621\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfad3f7ccf3450d840d379f8d9f387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002052752999588847, 'eval_runtime': 66.979, 'eval_samples_per_second': 307.873, 'eval_steps_per_second': 19.245, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-10500\n",
      "Configuration saved in ./results/checkpoint-10500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 1.1854150504266875e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-11000\n",
      "Configuration saved in ./results/checkpoint-11000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'learning_rate': 1.1466252909231963e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-11500\n",
      "Configuration saved in ./results/checkpoint-11500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'learning_rate': 1.1078355314197054e-05, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-12000\n",
      "Configuration saved in ./results/checkpoint-12000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0021, 'learning_rate': 1.0690457719162142e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-12500\n",
      "Configuration saved in ./results/checkpoint-12500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'learning_rate': 1.0302560124127233e-05, 'epoch': 2.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-13000\n",
      "Configuration saved in ./results/checkpoint-13000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0019, 'learning_rate': 9.91466252909232e-06, 'epoch': 2.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-13500\n",
      "Configuration saved in ./results/checkpoint-13500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 9.52676493405741e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-14000\n",
      "Configuration saved in ./results/checkpoint-14000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 9.138867339022499e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-14500\n",
      "Configuration saved in ./results/checkpoint-14500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0017, 'learning_rate': 8.750969743987588e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-15000\n",
      "Configuration saved in ./results/checkpoint-15000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 8.363072148952676e-06, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20621\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa18060f8f8e4d8493093cdce7c23b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0012002384755760431, 'eval_runtime': 66.9878, 'eval_samples_per_second': 307.832, 'eval_steps_per_second': 19.242, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-15500\n",
      "Configuration saved in ./results/checkpoint-15500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 7.975174553917767e-06, 'epoch': 3.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-16000\n",
      "Configuration saved in ./results/checkpoint-16000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 7.5872769588828555e-06, 'epoch': 3.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-16500\n",
      "Configuration saved in ./results/checkpoint-16500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 7.199379363847945e-06, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-17000\n",
      "Configuration saved in ./results/checkpoint-17000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'learning_rate': 6.811481768813034e-06, 'epoch': 3.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-17500\n",
      "Configuration saved in ./results/checkpoint-17500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 6.423584173778123e-06, 'epoch': 3.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-18000\n",
      "Configuration saved in ./results/checkpoint-18000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 6.035686578743212e-06, 'epoch': 3.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-18500\n",
      "Configuration saved in ./results/checkpoint-18500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'learning_rate': 5.647788983708301e-06, 'epoch': 3.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-19000\n",
      "Configuration saved in ./results/checkpoint-19000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 5.25989138867339e-06, 'epoch': 3.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-19500\n",
      "Configuration saved in ./results/checkpoint-19500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'learning_rate': 4.87199379363848e-06, 'epoch': 3.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-20000\n",
      "Configuration saved in ./results/checkpoint-20000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 4.484096198603569e-06, 'epoch': 3.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-20500\n",
      "Configuration saved in ./results/checkpoint-20500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'learning_rate': 4.096198603568658e-06, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20621\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae6a7837e174ed39d7683e49d7aab8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007637494127266109, 'eval_runtime': 67.1112, 'eval_samples_per_second': 307.266, 'eval_steps_per_second': 19.207, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-21000\n",
      "Configuration saved in ./results/checkpoint-21000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 3.708301008533747e-06, 'epoch': 4.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-21500\n",
      "Configuration saved in ./results/checkpoint-21500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 3.3204034134988368e-06, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-22000\n",
      "Configuration saved in ./results/checkpoint-22000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.932505818463926e-06, 'epoch': 4.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-22500\n",
      "Configuration saved in ./results/checkpoint-22500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 2.544608223429015e-06, 'epoch': 4.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-23000\n",
      "Configuration saved in ./results/checkpoint-23000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 2.156710628394104e-06, 'epoch': 4.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-23500\n",
      "Configuration saved in ./results/checkpoint-23500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.7688130333591933e-06, 'epoch': 4.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-24000\n",
      "Configuration saved in ./results/checkpoint-24000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 1.3809154383242826e-06, 'epoch': 4.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-24500\n",
      "Configuration saved in ./results/checkpoint-24500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 9.930178432893718e-07, 'epoch': 4.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-25000\n",
      "Configuration saved in ./results/checkpoint-25000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 6.051202482544609e-07, 'epoch': 4.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-25000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-25500\n",
      "Configuration saved in ./results/checkpoint-25500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.1722265321955006e-07, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-25500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20621\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4140eddc393f49e193257d11b1047f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0007221808773465455, 'eval_runtime': 67.0398, 'eval_samples_per_second': 307.594, 'eval_steps_per_second': 19.227, 'epoch': 5.0}\n",
      "{'train_runtime': 5315.1258, 'train_samples_per_second': 77.591, 'train_steps_per_second': 4.85, 'train_loss': 0.011468237978344837, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25780, training_loss=0.011468237978344837, metrics={'train_runtime': 5315.1258, 'train_samples_per_second': 77.591, 'train_steps_per_second': 4.85, 'train_loss': 0.011468237978344837, 'epoch': 5.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-clinical-scratch-wl-es-NER-prescription\n",
      "Configuration saved in bert-clinical-scratch-wl-es-NER-prescription/config.json\n",
      "Model weights saved in bert-clinical-scratch-wl-es-NER-prescription/pytorch_model.bin\n",
      "tokenizer config file saved in bert-clinical-scratch-wl-es-NER-prescription/tokenizer_config.json\n",
      "Special tokens file saved in bert-clinical-scratch-wl-es-NER-prescription/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"bert-clinical-scratch-wl-es-NER-prescription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auxfunctions import tokenize_and_align_labels\n",
    "\n",
    "MODEL = \"bert-clinical-scratch-wl-es-NER-prescription\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-clinical-scratch-wl-es-NER-prescription/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-clinical-scratch-wl-es-NER-prescription\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file bert-clinical-scratch-wl-es-NER-prescription/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-clinical-scratch-wl-es-NER-prescription.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxfunctions import eval_text, map_entities, calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  2,  3,  0,  0,  5,  7,  8,  8,  9, 10, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\"\n",
    "\n",
    "eval_text(text,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['B-ACTIVE_PRINCIPLE',\n",
       " 'I-ACTIVE_PRINCIPLE',\n",
       " 'I-ACTIVE_PRINCIPLE',\n",
       " 'B-FORMA_FARMA',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ADMIN',\n",
       " 'B-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'B-DURATION',\n",
       " 'I-DURATION',\n",
       " 'I-DURATION']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text)\n",
    "map_entities(eval_text(text,tokenizer,model),ner_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación con otras métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [row['ner_tags'] for row in HF_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progreso = 100.0%\n"
     ]
    }
   ],
   "source": [
    "y_preds = []\n",
    "length = len(HF_dataset['test'])\n",
    "for i, row in enumerate(HF_dataset['test']):\n",
    "    if 100*(i+1)/length % 10 == 0:\n",
    "        print(\"progreso = {}%\".format(100*(i+1)/length))\n",
    "    y_preds.append(list(eval_text(row['tokens'],tokenizer,model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas en test-etiquetado con ER\n",
      "Resultados de evaluación\n",
      "\t f1: 1.00 | precision: 1.00 | recall: 1.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9995104807330237, 0.9997614563716258, 0.9996359527993973)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Métricas en test-etiquetado con ER\")\n",
    "calculate_metrics(y_preds,y_test,ner_dict=ner_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 250 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "datos_conll = Corpus()\n",
    "datos_conll.entidades = ner_dict\n",
    "\n",
    "for i in range(4):\n",
    "    datos_conll.load_conll('../datos/Etiquetado/corpus_s{}_etiquetados.conll'.format(i+1))\n",
    "\n",
    "HF_data_mini = datos_conll.to_HF_dataset()\n",
    "HF_dataset_mini = HF_data_mini.train_test_split(test_size=0.2,seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas en test etiquetado manual\n",
      "Resultados de evaluación\n",
      "\t f1: 0.30 | precision: 0.32 | recall: 0.29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.32114882506527415, 0.28771929824561404, 0.3035163479333745)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auxfunctions import eval_text, map_entities, calculate_metrics\n",
    "\n",
    "y_test_em = [row['ner_tags'] for row in HF_dataset_mini['test']]\n",
    "y_preds_em = [list(eval_text(row['tokens'],tokenizer,model)) for row in HF_dataset_mini['test']]\n",
    "\n",
    "print(\"Métricas en test etiquetado manual\")\n",
    "calculate_metrics(y_preds_em,y_test_em,ner_dict=ner_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versión mini\n",
    "\n",
    "A continuación definimos un modelo como el anterior, con la diferencia de que este será entrenado (fine-tunning) en 80% de los datos etiquetados a mano y testeado con 20 % de estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../datos/procesamiento')\n",
    "from corpus import Corpus\n",
    "\n",
    "datos_conll = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }\n",
    "\n",
    "datos_conll.entidades = ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 250 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    datos_conll.load_conll('../datos/Etiquetado/corpus_s{}_etiquetados.conll'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 802\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 201\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_data_mini = datos_conll.to_HF_dataset()\n",
    "\n",
    "HF_dataset = HF_data_mini.train_test_split(test_size=0.2,seed=0)\n",
    "HF_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/camilo/.cache/huggingface/hub/models--plncmm--bert-clinical-scratch-wl-es/snapshots/e9314ad921431b0c0b69c84149ce3dfe5810b324/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/camilo/.cache/huggingface/hub/models--plncmm--bert-clinical-scratch-wl-es/snapshots/e9314ad921431b0c0b69c84149ce3dfe5810b324/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/camilo/.cache/huggingface/hub/models--plncmm--bert-clinical-scratch-wl-es/snapshots/e9314ad921431b0c0b69c84149ce3dfe5810b324/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/camilo/.cache/huggingface/hub/models--plncmm--bert-clinical-scratch-wl-es/snapshots/e9314ad921431b0c0b69c84149ce3dfe5810b324/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58831b2fc51746c89fb11ff84b514fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa7a4e11a554f1991f4997b127240a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auxfunctions import tokenize_and_align_labels\n",
    "\n",
    "MODEL = \"plncmm/bert-clinical-scratch-wl-es\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "process = lambda examples: tokenize_and_align_labels(examples,tokenizer)\n",
    "\n",
    "tokenized_data_mini = HF_dataset.map(process, batched=True)\n",
    "tokenized_data_mini = tokenized_data_mini.remove_columns(['id','tokens','ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/camilo/.cache/huggingface/hub/models--plncmm--bert-clinical-scratch-wl-es/snapshots/e9314ad921431b0c0b69c84149ce3dfe5810b324/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"plncmm/bert-clinical-scratch-wl-es\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/camilo/.cache/huggingface/hub/models--plncmm--bert-clinical-scratch-wl-es/snapshots/e9314ad921431b0c0b69c84149ce3dfe5810b324/pytorch_model.bin\n",
      "Some weights of the model checkpoint at plncmm/bert-clinical-scratch-wl-es were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at plncmm/bert-clinical-scratch-wl-es and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model_mini = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=len(ner_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 20,\n",
    "    weight_decay = 0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_mini,\n",
    "    args=training_args,\n",
    "    train_dataset = tokenized_data_mini[\"train\"],\n",
    "    eval_dataset = tokenized_data_mini[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 802\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db102df539344739d186056115be712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa737c6a1d94656baf906a4a373698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5831220746040344, 'eval_runtime': 0.5841, 'eval_samples_per_second': 344.145, 'eval_steps_per_second': 22.258, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15a61233b324b059f79664b91872bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42039012908935547, 'eval_runtime': 0.5968, 'eval_samples_per_second': 336.817, 'eval_steps_per_second': 21.784, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3634359d715a4f6380035e247fa07a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.384698748588562, 'eval_runtime': 0.596, 'eval_samples_per_second': 337.243, 'eval_steps_per_second': 21.812, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014a1460ec1f4c559e4f1befe1d021f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3523677885532379, 'eval_runtime': 0.5796, 'eval_samples_per_second': 346.783, 'eval_steps_per_second': 22.429, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995ba0063c4c4a3392b8e0e9a9aead09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32841044664382935, 'eval_runtime': 0.5911, 'eval_samples_per_second': 340.042, 'eval_steps_per_second': 21.993, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1d4bba30834759b3b4d61596934315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3156881630420685, 'eval_runtime': 0.6252, 'eval_samples_per_second': 321.495, 'eval_steps_per_second': 20.793, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10b0790bc694801a199bede13219377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32430407404899597, 'eval_runtime': 0.6249, 'eval_samples_per_second': 321.671, 'eval_steps_per_second': 20.805, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ee190ca6934b7899f613051cc3f0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3156029284000397, 'eval_runtime': 0.6059, 'eval_samples_per_second': 331.724, 'eval_steps_per_second': 21.455, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9a3f1cb89e433ab70ab42d62ae94d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3170432150363922, 'eval_runtime': 0.5995, 'eval_samples_per_second': 335.294, 'eval_steps_per_second': 21.686, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3258, 'learning_rate': 1.0196078431372549e-05, 'epoch': 9.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5d1fd23ba24ea7a64ed9852b595d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3161526918411255, 'eval_runtime': 0.6586, 'eval_samples_per_second': 305.212, 'eval_steps_per_second': 19.74, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b876f8f2accc4f13a00570968e7b0d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33779987692832947, 'eval_runtime': 0.6436, 'eval_samples_per_second': 312.303, 'eval_steps_per_second': 20.199, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5124a88610fa4e419f1417e006023691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34254539012908936, 'eval_runtime': 0.645, 'eval_samples_per_second': 311.611, 'eval_steps_per_second': 20.154, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44d3081951741c8b7310eb52ade460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3367880880832672, 'eval_runtime': 0.6175, 'eval_samples_per_second': 325.488, 'eval_steps_per_second': 21.051, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f306170ada46b4bd3a127190d63d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34026771783828735, 'eval_runtime': 0.6365, 'eval_samples_per_second': 315.813, 'eval_steps_per_second': 20.426, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b68d6a17c74d6385a837cf5e79023d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3516131341457367, 'eval_runtime': 0.6438, 'eval_samples_per_second': 312.217, 'eval_steps_per_second': 20.193, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe19b0828ea43ee8b5ba1210bd4e278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3546003997325897, 'eval_runtime': 0.6061, 'eval_samples_per_second': 331.639, 'eval_steps_per_second': 21.449, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da37f8f68b874ca99559a4b2f3c155b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34490272402763367, 'eval_runtime': 0.6138, 'eval_samples_per_second': 327.448, 'eval_steps_per_second': 21.178, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfce0ebd1624d8bb56be2d659c4f2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3546936511993408, 'eval_runtime': 0.6192, 'eval_samples_per_second': 324.602, 'eval_steps_per_second': 20.994, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f5ee2bb7324078ac66e2c06aab2e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3440197706222534, 'eval_runtime': 0.6194, 'eval_samples_per_second': 324.505, 'eval_steps_per_second': 20.988, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0749, 'learning_rate': 3.921568627450981e-07, 'epoch': 19.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d2567c2b7f405a81a9f2bffd092f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34232205152511597, 'eval_runtime': 0.6261, 'eval_samples_per_second': 321.058, 'eval_steps_per_second': 20.765, 'epoch': 20.0}\n",
      "{'train_runtime': 199.0388, 'train_samples_per_second': 80.587, 'train_steps_per_second': 5.125, 'train_loss': 0.1973839650551478, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1020, training_loss=0.1973839650551478, metrics={'train_runtime': 199.0388, 'train_samples_per_second': 80.587, 'train_steps_per_second': 5.125, 'train_loss': 0.1973839650551478, 'epoch': 20.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-clinical-scratch-wl-es-NER-prescription-mini\n",
      "Configuration saved in bert-clinical-scratch-wl-es-NER-prescription-mini/config.json\n",
      "Model weights saved in bert-clinical-scratch-wl-es-NER-prescription-mini/pytorch_model.bin\n",
      "tokenizer config file saved in bert-clinical-scratch-wl-es-NER-prescription-mini/tokenizer_config.json\n",
      "Special tokens file saved in bert-clinical-scratch-wl-es-NER-prescription-mini/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"bert-clinical-scratch-wl-es-NER-prescription-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-clinical-scratch-wl-es-NER-prescription-mini/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-clinical-scratch-wl-es-NER-prescription-mini\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file bert-clinical-scratch-wl-es-NER-prescription-mini/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-clinical-scratch-wl-es-NER-prescription-mini.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_mini = AutoModelForTokenClassification.from_pretrained(\"bert-clinical-scratch-wl-es-NER-prescription-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluación\n",
      "\t f1: 0.86 | precision: 0.83 | recall: 0.90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8270676691729323, 0.9005847953216374, 0.8622620380739081)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auxfunctions import eval_text, map_entities, calculate_metrics\n",
    "\n",
    "y_test = [row['ner_tags'] for row in HF_dataset['test']]\n",
    "y_preds = [list(eval_text(row['tokens'],tokenizer,model_mini)) for row in HF_dataset['test']]\n",
    "\n",
    "calculate_metrics(y_preds,y_test,ner_dict=ner_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versión fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03df2fadf95546a7be8b273019fbd7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3858999f3c51406c80f5a83eb29fe240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-clinical-scratch-wl-es-NER-prescription/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-clinical-scratch-wl-es-NER-prescription\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file bert-clinical-scratch-wl-es-NER-prescription/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-clinical-scratch-wl-es-NER-prescription.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"bert-clinical-scratch-wl-es-NER-prescription\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "process = lambda examples: tokenize_and_align_labels(examples,tokenizer)\n",
    "\n",
    "tokenized_data_mini = HF_dataset.map(process, batched=True)\n",
    "tokenized_data_mini = tokenized_data_mini.remove_columns(['id','tokens','ner_tags'])\n",
    "\n",
    "model_mini = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=len(ner_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 20,\n",
    "    weight_decay = 0.01,\n",
    ")\n",
    "\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_mini,\n",
    "    args=training_args,\n",
    "    train_dataset = tokenized_data_mini[\"train\"],\n",
    "    eval_dataset = tokenized_data_mini[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 802\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180f5563a8404926927275b0024a6595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1020 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4f77a207a4420390f42983fd9d7355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3356422781944275, 'eval_runtime': 0.5573, 'eval_samples_per_second': 360.646, 'eval_steps_per_second': 23.325, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10fee3dbbd4408db22cded77101c46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28154703974723816, 'eval_runtime': 0.5674, 'eval_samples_per_second': 354.252, 'eval_steps_per_second': 22.912, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cb566067154c06ba7994aa21454d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28420379757881165, 'eval_runtime': 0.5723, 'eval_samples_per_second': 351.208, 'eval_steps_per_second': 22.715, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11926e23f6d44b79b05a39b6d7cd98e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26701831817626953, 'eval_runtime': 0.579, 'eval_samples_per_second': 347.155, 'eval_steps_per_second': 22.453, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd11868d8ac9459bb8d5d9c53c13e143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2742534577846527, 'eval_runtime': 0.5846, 'eval_samples_per_second': 343.846, 'eval_steps_per_second': 22.239, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8ee4a945a34409bdc7513a595f8fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28834789991378784, 'eval_runtime': 0.5871, 'eval_samples_per_second': 342.349, 'eval_steps_per_second': 22.142, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5afcb025edc4dd7ada1b5aba16602a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30232951045036316, 'eval_runtime': 0.5916, 'eval_samples_per_second': 339.731, 'eval_steps_per_second': 21.973, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac227e6596ba43d48e093d54a5dddca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2897992730140686, 'eval_runtime': 0.5946, 'eval_samples_per_second': 338.023, 'eval_steps_per_second': 21.862, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7afa7d86bc48558bea186604232166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3276868164539337, 'eval_runtime': 0.5951, 'eval_samples_per_second': 337.761, 'eval_steps_per_second': 21.845, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2139, 'learning_rate': 1.0196078431372549e-05, 'epoch': 9.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448ce741be1b4992aa8d3878404dc107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34667789936065674, 'eval_runtime': 0.5975, 'eval_samples_per_second': 336.397, 'eval_steps_per_second': 21.757, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e9543a6816457a81101cc3b28b3642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3635108172893524, 'eval_runtime': 0.5981, 'eval_samples_per_second': 336.077, 'eval_steps_per_second': 21.736, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb745ccc14cb4c44a8c8f0aa703ad868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3986039161682129, 'eval_runtime': 0.598, 'eval_samples_per_second': 336.14, 'eval_steps_per_second': 21.74, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac558fe62e414addb9b541e0f18d3b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.398256778717041, 'eval_runtime': 0.6004, 'eval_samples_per_second': 334.789, 'eval_steps_per_second': 21.653, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d78dfd3a7b44c5abeb2a73b3ee760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3979812264442444, 'eval_runtime': 0.6061, 'eval_samples_per_second': 331.654, 'eval_steps_per_second': 21.45, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f94f4e7f9f493b8d54d381ffb75357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41253241896629333, 'eval_runtime': 0.6064, 'eval_samples_per_second': 331.44, 'eval_steps_per_second': 21.436, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e91ec331f2437a9d5aa98cf5b5909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41532662510871887, 'eval_runtime': 0.6085, 'eval_samples_per_second': 330.303, 'eval_steps_per_second': 21.363, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6160801386c24a08b139b5c85ec13d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4188380539417267, 'eval_runtime': 0.6114, 'eval_samples_per_second': 328.734, 'eval_steps_per_second': 21.261, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e643ba5502a34ff0872534e2d84803cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4327171742916107, 'eval_runtime': 0.6103, 'eval_samples_per_second': 329.339, 'eval_steps_per_second': 21.301, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a077689132482ba703f11c8fb3a030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4252334535121918, 'eval_runtime': 0.6135, 'eval_samples_per_second': 327.603, 'eval_steps_per_second': 21.188, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0189, 'learning_rate': 3.921568627450981e-07, 'epoch': 19.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 201\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38ec194d9c14357b5b4af272c56fbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42849618196487427, 'eval_runtime': 0.6127, 'eval_samples_per_second': 328.075, 'eval_steps_per_second': 21.219, 'epoch': 20.0}\n",
      "{'train_runtime': 193.1858, 'train_samples_per_second': 83.029, 'train_steps_per_second': 5.28, 'train_loss': 0.11428771498156529, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1020, training_loss=0.11428771498156529, metrics={'train_runtime': 193.1858, 'train_samples_per_second': 83.029, 'train_steps_per_second': 5.28, 'train_loss': 0.11428771498156529, 'epoch': 20.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-clinical-scratch-wl-es-NER-prescription-mini\n",
      "Configuration saved in bert-clinical-scratch-wl-es-NER-prescription-mini/config.json\n",
      "Model weights saved in bert-clinical-scratch-wl-es-NER-prescription-mini/pytorch_model.bin\n",
      "tokenizer config file saved in bert-clinical-scratch-wl-es-NER-prescription-mini/tokenizer_config.json\n",
      "Special tokens file saved in bert-clinical-scratch-wl-es-NER-prescription-mini/special_tokens_map.json\n",
      "loading configuration file bert-clinical-scratch-wl-es-NER-prescription-mini/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-clinical-scratch-wl-es-NER-prescription-mini\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file bert-clinical-scratch-wl-es-NER-prescription-mini/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-clinical-scratch-wl-es-NER-prescription-mini.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluación\n",
      "\t f1: 0.93 | precision: 0.92 | recall: 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9220183486238532, 0.9403508771929825, 0.9310943833236828)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"bert-clinical-scratch-wl-es-NER-prescription-mini\")\n",
    "\n",
    "model_mini = AutoModelForTokenClassification.from_pretrained(\"bert-clinical-scratch-wl-es-NER-prescription-mini\")\n",
    "\n",
    "from auxfunctions import eval_text, map_entities, calculate_metrics\n",
    "\n",
    "y_test = [row['ner_tags'] for row in HF_dataset['test']]\n",
    "y_preds = [list(eval_text(row['tokens'],tokenizer,model_mini)) for row in HF_dataset['test']]\n",
    "\n",
    "calculate_metrics(y_preds,y_test,ner_dict=ner_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('entidades_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60cab8a6eccdbb291cd8546425070b6bbfbad3cae25f59923f3919ec9d23e6db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
