{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de tokens con HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../datos/procesamiento')\n",
    "from corpus import Corpus\n",
    "from etiquetado_entidades import codigo_hfl, split_rule, process_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_db = pd.read_csv('../datos/DATA_HLF_MDS_2.csv',sep=',')\n",
    "\n",
    "code_db = pd.read_excel('../datos/PRINCIPIOS_ACTIVOS_MDS.xlsx')\n",
    "HLF = code_db.loc[:,['PRINCIPIO_ACTIVO','CODIGO_HLF']]\n",
    "HLF_df = codigo_hfl(HLF)\n",
    "\n",
    "df = main_db.join(HLF_df.set_index('CODIGO_MEDICAMENTO'), on='CODIGO_MEDICAMENTO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar principios activos, forma farma y juntar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARA DETECTAR LA POSICION DE ENTIDADES TIPO PRINCIPIO ACTIVO, FORMA FARMACO SE COMPARARAN LOS ELEMENTOS DEL CORPUS CON LAS LISTAS CORRESPONDIENTES.\n",
    "PA = np.unique(split_rule(df['PRINCIPIO_ACTIVO'].dropna().unique()))\n",
    "FF = np.unique(split_rule(df['FORMA_FARMA'].dropna().unique()))\n",
    "\n",
    "rows = (df['PRES_DENOMINACION'] + ' ' + df['RESUMEN']).dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar procesamiento de datos de Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()\n",
    "\n",
    "for i in range(len(rows)):\n",
    "    row = rows[i].split()\n",
    "    tagged_seq = process_column(row,PA,FF)\n",
    "\n",
    "    corpus.append(tagged_seq)\n",
    "\n",
    "    if i > 100:\n",
    "        # solo testeando :)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cargando el corpus como dataset de HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTVPRNCP': 1,\n",
    "            'I-ACTVPRNCP': 2,\n",
    "            'B-ADMIN': 3,\n",
    "            'I-ADMIN': 4,\n",
    "            'B-PERIODICITY': 5,\n",
    "            'I-PERIODICITY': 6,\n",
    "            'B-DURATION': 7,\n",
    "            'I-DURATION': 8\n",
    "            }\n",
    "\n",
    "corpus.entidades = ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_dataset = corpus.to_HF_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryTable\n",
       "id: int64\n",
       "tokens: list<item: string>\n",
       "  child 0, item: string\n",
       "ner_tags: list<item: int64>\n",
       "  child 0, item: int64\n",
       "----\n",
       "id: [[0,1,2,3,4,...,97,98,99,100,101]]\n",
       "tokens: [[[\"PARACETAMOL\",\"500\",\"MG\",\"COMPRIMIDO\",\"1\",...,\"6\",\"horas\",\"durante\",\"3\",\"dias\"],[\"KETOROLACO\",\"10\",\"MG\",\"COMPRIMIDO\",\"1\",...,\"8\",\"horas\",\"durante\",\"3\",\"dias\"],...,[\"KETOPROFENO\",\"50\",\"MG\",\"CÁPSULA\",\"1\",...,\"ORAL\",\"tres\",\"veces\",\"al\",\"día\"],[\"DICLOFENACO\",\"75\",\"MG/3\",\"ML\",\"SOLUCIÓN\",...,\"1\",\"UNIDAD\",\"INTRAMUSCULAR\",\"dosis\",\"simple\"]]]\n",
       "ner_tags: [[[1,0,0,3,0,...,6,6,7,8,8],[1,0,0,3,0,...,6,6,7,8,8],...,[1,0,0,3,0,...,3,0,0,0,0],[1,0,0,0,1,...,0,0,0,0,0]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ACTVPRNCP',\n",
       " 'I-ACTVPRNCP',\n",
       " 'B-ADMIN',\n",
       " 'I-ADMIN',\n",
       " 'B-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'B-DURATION',\n",
       " 'I-DURATION']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset.features[f\"ner_tags\"].feature.names = [key for key in ner_dict.keys()]\n",
    "HF_dataset.features[f\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 81\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset = HF_dataset.train_test_split(test_size=0.2,seed=0)\n",
    "HF_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos un modelo basado en Transformers de HuggingFace\n",
    "\n",
    "Nuestro modelo será: plncmm/bert-clinical-scratch-wl-es. Ha sido fine-tuneado con texto médico (aunque probablemente no con prescripciones)\n",
    "\n",
    "Fuente de estasección ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"plncmm/bert-clinical-scratch-wl-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el tokenizador para codificar nuestro input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'meta',\n",
       " '##mi',\n",
       " '##zo',\n",
       " '##l',\n",
       " 'só',\n",
       " '##dico',\n",
       " '1',\n",
       " 'g',\n",
       " '/',\n",
       " '2',\n",
       " 'ml',\n",
       " 'solución',\n",
       " 'inyecta',\n",
       " '##ble',\n",
       " 'amp',\n",
       " '##olla',\n",
       " '2',\n",
       " 'ml',\n",
       " '4',\n",
       " 'unidad',\n",
       " 'paren',\n",
       " '##tera',\n",
       " '##l',\n",
       " 'diaria',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = HF_dataset[\"train\"][0]\n",
    "\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos corregir el _mismatch_ entre input tokenizado y la lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('entidades_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60cab8a6eccdbb291cd8546425070b6bbfbad3cae25f59923f3919ec9d23e6db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
