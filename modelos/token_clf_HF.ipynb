{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de tokens con HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../datos/procesamiento')\n",
    "from corpus import Corpus\n",
    "\n",
    "corpus_train = Corpus()\n",
    "corpus_test = Corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cargando el corpus como dataset de HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }\n",
    "\n",
    "corpus_train.entidades = ner_dict\n",
    "corpus_test.entidades = ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 85246 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_train.load_conll('../datos/Etiquetado/corpus_ER_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 51183 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "corpus_train.load_conll('../datos/Etiquetado/corpus_ER_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "HF_dataset = DatasetDict()\n",
    "\n",
    "HF_dataset['train'] = corpus_train.to_HF_dataset()\n",
    "HF_dataset['test'] = corpus_test.to_HF_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 136429\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 136429\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos un modelo basado en Transformers de HuggingFace\n",
    "\n",
    "Nuestro modelo será: plncmm/bert-clinical-scratch-wl-es. Ha sido fine-tuneado con texto médico (aunque probablemente no con prescripciones)\n",
    "\n",
    "[Fuente de esta sección](https://huggingface.co/docs/transformers/tasks/token_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"plncmm/bert-clinical-scratch-wl-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el tokenizador para codificar nuestro input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'flu',\n",
       " '##cona',\n",
       " '##zo',\n",
       " '##l',\n",
       " '150',\n",
       " 'mg',\n",
       " 'cápsula',\n",
       " '1',\n",
       " 'cápsula',\n",
       " 'oral',\n",
       " 'cada',\n",
       " '3',\n",
       " 'dias',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = HF_dataset[\"train\"][0]\n",
    "\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos corregir el _mismatch_ entre input tokenizado y la lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from auxfunctions import tokenize_and_align_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4190835b5843480dbe9e3a731d0fe6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15842bb4e5674253a86b0ff47934788e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = HF_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_data = tokenized_data.remove_columns(['id','tokens','ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': InMemoryTable\n",
       " input_ids: list<item: int32>\n",
       "   child 0, item: int32\n",
       " token_type_ids: list<item: int8>\n",
       "   child 0, item: int8\n",
       " attention_mask: list<item: int8>\n",
       "   child 0, item: int8\n",
       " labels: list<item: int64>\n",
       "   child 0, item: int64\n",
       " ----\n",
       " input_ids: [[[4,5121,30534,1600,30962,...,12791,1748,1306,12873,5],[4,11236,1348,29117,3284,...,1139,28376,30967,30972,5],...,[4,11247,13436,15275,30956,...,2581,29258,12791,15359,5],[4,8753,3283,2861,22396,...,1411,2242,1074,1707,5]],[[4,15023,13394,1177,1114,...,2596,1672,1129,12873,5],[4,5970,4358,1167,1600,...,1707,1672,1129,12873,5],...,[4,11236,1348,29117,3284,...,29258,12791,1074,23938,5],[4,27625,11425,30960,6821,...,1139,1992,1482,30961,5]],...,[[4,4667,27921,19257,15023,...,1411,2242,1074,1707,5],[4,15721,13067,10791,997,...,2596,1672,1413,12873,5],...,[4,1254,1092,1104,10110,...,2596,1672,1306,2817,5],[4,16768,1307,1275,1316,...,989,1129,1139,1098,5]],[[4,9969,13908,1519,30967,...,15126,2155,9934,2940,5],[4,13695,13456,16387,15275,...,4205,28982,1976,15359,5],...,[4,1408,18100,9700,14194,...,12791,1748,1098,12873,5],[4,5]]]\n",
       " token_type_ids: [[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],...,[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0]]]\n",
       " attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],...,[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1]]]\n",
       " labels: [[[-100,1,-100,-100,-100,...,5,7,8,8,-100],[-100,1,-100,-100,0,...,0,0,-100,-100,-100],...,[-100,1,-100,-100,-100,...,-100,0,5,7,-100],[-100,1,-100,-100,-100,...,7,8,8,8,-100]],[[-100,1,-100,-100,-100,...,8,9,10,10,-100],[-100,0,-100,-100,-100,...,8,9,10,10,-100],...,[-100,1,-100,-100,0,...,0,5,7,8,-100],[-100,1,2,-100,-100,...,-100,-100,0,-100,-100]],...,[[-100,1,2,-100,2,...,7,8,8,8,-100],[-100,0,-100,-100,0,...,8,9,10,10,-100],...,[-100,1,-100,-100,-100,...,8,0,0,0,-100],[-100,1,-100,-100,-100,...,-100,-100,-100,-100,-100]],[[-100,1,-100,-100,-100,...,-100,-100,0,0,-100],[-100,1,-100,-100,-100,...,0,5,-100,7,-100],...,[-100,1,-100,-100,-100,...,5,7,8,8,-100],[-100,-100]]],\n",
       " 'test': InMemoryTable\n",
       " input_ids: list<item: int32>\n",
       "   child 0, item: int32\n",
       " token_type_ids: list<item: int8>\n",
       "   child 0, item: int8\n",
       " attention_mask: list<item: int8>\n",
       "   child 0, item: int8\n",
       " labels: list<item: int64>\n",
       "   child 0, item: int64\n",
       " ----\n",
       " input_ids: [[[4,5121,30534,1600,30962,...,12791,1748,1306,12873,5],[4,11236,1348,29117,3284,...,1139,28376,30967,30972,5],...,[4,11247,13436,15275,30956,...,2581,29258,12791,15359,5],[4,8753,3283,2861,22396,...,1411,2242,1074,1707,5]],[[4,15023,13394,1177,1114,...,2596,1672,1129,12873,5],[4,5970,4358,1167,1600,...,1707,1672,1129,12873,5],...,[4,11236,1348,29117,3284,...,29258,12791,1074,23938,5],[4,27625,11425,30960,6821,...,1139,1992,1482,30961,5]],...,[[4,4667,27921,19257,15023,...,1411,2242,1074,1707,5],[4,15721,13067,10791,997,...,2596,1672,1413,12873,5],...,[4,1254,1092,1104,10110,...,2596,1672,1306,2817,5],[4,16768,1307,1275,1316,...,989,1129,1139,1098,5]],[[4,9969,13908,1519,30967,...,15126,2155,9934,2940,5],[4,13695,13456,16387,15275,...,4205,28982,1976,15359,5],...,[4,1408,18100,9700,14194,...,12791,1748,1098,12873,5],[4,5]]]\n",
       " token_type_ids: [[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],...,[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0]]]\n",
       " attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],...,[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1]]]\n",
       " labels: [[[-100,1,-100,-100,-100,...,5,7,8,8,-100],[-100,1,-100,-100,0,...,0,0,-100,-100,-100],...,[-100,1,-100,-100,-100,...,-100,0,5,7,-100],[-100,1,-100,-100,-100,...,7,8,8,8,-100]],[[-100,1,-100,-100,-100,...,8,9,10,10,-100],[-100,0,-100,-100,-100,...,8,9,10,10,-100],...,[-100,1,-100,-100,0,...,0,5,7,8,-100],[-100,1,2,-100,-100,...,-100,-100,0,-100,-100]],...,[[-100,1,2,-100,2,...,7,8,8,8,-100],[-100,0,-100,-100,0,...,8,9,10,10,-100],...,[-100,1,-100,-100,-100,...,8,0,0,0,-100],[-100,1,-100,-100,-100,...,-100,-100,-100,-100,-100]],[[-100,1,-100,-100,-100,...,-100,-100,0,0,-100],[-100,1,-100,-100,-100,...,0,5,-100,7,-100],...,[-100,1,-100,-100,-100,...,5,7,8,8,-100],[-100,-100]]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at plncmm/bert-clinical-scratch-wl-es were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at plncmm/bert-clinical-scratch-wl-es and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=len(ner_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset = tokenized_data[\"train\"],\n",
    "    eval_dataset = tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 136429\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 42635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256b5c36141747fbb7ccfaa26460c72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2023, 'learning_rate': 1.9765450920605137e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0551, 'learning_rate': 1.9530901841210276e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0347, 'learning_rate': 1.929635276181541e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0257, 'learning_rate': 1.9061803682420547e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0176, 'learning_rate': 1.8827254603025685e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0135, 'learning_rate': 1.859270552363082e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0133, 'learning_rate': 1.8358156444235956e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0097, 'learning_rate': 1.8123607364841095e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0099, 'learning_rate': 1.788905828544623e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0082, 'learning_rate': 1.7654509206051366e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0091, 'learning_rate': 1.7419960126656504e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0066, 'learning_rate': 1.7185411047261643e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-6500\n",
      "Configuration saved in ./results/checkpoint-6500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.006, 'learning_rate': 1.695086196786678e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-7000\n",
      "Configuration saved in ./results/checkpoint-7000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0059, 'learning_rate': 1.6716312888471914e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-7500\n",
      "Configuration saved in ./results/checkpoint-7500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0049, 'learning_rate': 1.6481763809077053e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-8000\n",
      "Configuration saved in ./results/checkpoint-8000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0048, 'learning_rate': 1.6247214729682188e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-8500\n",
      "Configuration saved in ./results/checkpoint-8500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0052, 'learning_rate': 1.6012665650287323e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 136429\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771a1294246b4c3889c5c2e48f78a189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8527 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0027356892824172974, 'eval_runtime': 439.9435, 'eval_samples_per_second': 310.106, 'eval_steps_per_second': 19.382, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-9000\n",
      "Configuration saved in ./results/checkpoint-9000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0037, 'learning_rate': 1.5778116570892462e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-9500\n",
      "Configuration saved in ./results/checkpoint-9500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'learning_rate': 1.5543567491497598e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-10000\n",
      "Configuration saved in ./results/checkpoint-10000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'learning_rate': 1.5309018412102733e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-10500\n",
      "Configuration saved in ./results/checkpoint-10500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.005, 'learning_rate': 1.507446933270787e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-11000\n",
      "Configuration saved in ./results/checkpoint-11000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'learning_rate': 1.4839920253313007e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-11500\n",
      "Configuration saved in ./results/checkpoint-11500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.003, 'learning_rate': 1.4605371173918144e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-12000\n",
      "Configuration saved in ./results/checkpoint-12000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0034, 'learning_rate': 1.437082209452328e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-12500\n",
      "Configuration saved in ./results/checkpoint-12500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0028, 'learning_rate': 1.4136273015128417e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-13000\n",
      "Configuration saved in ./results/checkpoint-13000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'learning_rate': 1.3901723935733554e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-13500\n",
      "Configuration saved in ./results/checkpoint-13500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'learning_rate': 1.3667174856338689e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-14000\n",
      "Configuration saved in ./results/checkpoint-14000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0025, 'learning_rate': 1.3432625776943826e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-14500\n",
      "Configuration saved in ./results/checkpoint-14500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'learning_rate': 1.3198076697548963e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-15000\n",
      "Configuration saved in ./results/checkpoint-15000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0025, 'learning_rate': 1.2963527618154099e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-15500\n",
      "Configuration saved in ./results/checkpoint-15500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0028, 'learning_rate': 1.2728978538759236e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-16000\n",
      "Configuration saved in ./results/checkpoint-16000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 1.2494429459364373e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-16500\n",
      "Configuration saved in ./results/checkpoint-16500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0035, 'learning_rate': 1.2259880379969508e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-17000\n",
      "Configuration saved in ./results/checkpoint-17000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'learning_rate': 1.2025331300574645e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 136429\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122f38c6a2214510b0317e23a63885e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8527 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0011624422622844577, 'eval_runtime': 440.8558, 'eval_samples_per_second': 309.464, 'eval_steps_per_second': 19.342, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-17500\n",
      "Configuration saved in ./results/checkpoint-17500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'learning_rate': 1.1790782221179782e-05, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-18000\n",
      "Configuration saved in ./results/checkpoint-18000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 1.1556233141784921e-05, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-18500\n",
      "Configuration saved in ./results/checkpoint-18500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 1.1321684062390057e-05, 'epoch': 2.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-19000\n",
      "Configuration saved in ./results/checkpoint-19000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'learning_rate': 1.1087134982995194e-05, 'epoch': 2.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-19500\n",
      "Configuration saved in ./results/checkpoint-19500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 1.085258590360033e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-20000\n",
      "Configuration saved in ./results/checkpoint-20000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0023, 'learning_rate': 1.0618036824205466e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-20500\n",
      "Configuration saved in ./results/checkpoint-20500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 1.0383487744810603e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-21000\n",
      "Configuration saved in ./results/checkpoint-21000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 1.014893866541574e-05, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-21500\n",
      "Configuration saved in ./results/checkpoint-21500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 9.914389586020876e-06, 'epoch': 2.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-22000\n",
      "Configuration saved in ./results/checkpoint-22000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 9.679840506626013e-06, 'epoch': 2.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-22500\n",
      "Configuration saved in ./results/checkpoint-22500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'learning_rate': 9.44529142723115e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-23000\n",
      "Configuration saved in ./results/checkpoint-23000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 9.210742347836285e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-23500\n",
      "Configuration saved in ./results/checkpoint-23500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'learning_rate': 8.976193268441422e-06, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-24000\n",
      "Configuration saved in ./results/checkpoint-24000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 8.74164418904656e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-24500\n",
      "Configuration saved in ./results/checkpoint-24500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 8.507095109651695e-06, 'epoch': 2.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-25000\n",
      "Configuration saved in ./results/checkpoint-25000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 8.272546030256832e-06, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-25000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-25500\n",
      "Configuration saved in ./results/checkpoint-25500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'learning_rate': 8.037996950861969e-06, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-25500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 136429\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f4486a34394939bd64a7075077dbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8527 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0005563591257669032, 'eval_runtime': 439.4956, 'eval_samples_per_second': 310.422, 'eval_steps_per_second': 19.402, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-26000\n",
      "Configuration saved in ./results/checkpoint-26000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 7.803447871467104e-06, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-26500\n",
      "Configuration saved in ./results/checkpoint-26500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 7.568898792072241e-06, 'epoch': 3.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-26500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-27000\n",
      "Configuration saved in ./results/checkpoint-27000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 7.334349712677379e-06, 'epoch': 3.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-27000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-27500\n",
      "Configuration saved in ./results/checkpoint-27500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 7.099800633282515e-06, 'epoch': 3.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-27500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-28000\n",
      "Configuration saved in ./results/checkpoint-28000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 6.865251553887652e-06, 'epoch': 3.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-28500\n",
      "Configuration saved in ./results/checkpoint-28500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'learning_rate': 6.630702474492789e-06, 'epoch': 3.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-29000\n",
      "Configuration saved in ./results/checkpoint-29000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 6.396153395097925e-06, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-29000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-29500\n",
      "Configuration saved in ./results/checkpoint-29500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 6.161604315703061e-06, 'epoch': 3.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-29500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-30000\n",
      "Configuration saved in ./results/checkpoint-30000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 5.927055236308198e-06, 'epoch': 3.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-30000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-30500\n",
      "Configuration saved in ./results/checkpoint-30500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0008, 'learning_rate': 5.6925061569133345e-06, 'epoch': 3.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-31000\n",
      "Configuration saved in ./results/checkpoint-31000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 5.457957077518471e-06, 'epoch': 3.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-31000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-31500\n",
      "Configuration saved in ./results/checkpoint-31500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 5.223407998123608e-06, 'epoch': 3.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-31500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-32000\n",
      "Configuration saved in ./results/checkpoint-32000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 4.988858918728745e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-32000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-32500\n",
      "Configuration saved in ./results/checkpoint-32500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 4.754309839333881e-06, 'epoch': 3.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-32500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-33000\n",
      "Configuration saved in ./results/checkpoint-33000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 4.519760759939017e-06, 'epoch': 3.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-33000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-33500\n",
      "Configuration saved in ./results/checkpoint-33500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 4.285211680544154e-06, 'epoch': 3.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-33500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-34000\n",
      "Configuration saved in ./results/checkpoint-34000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 4.050662601149291e-06, 'epoch': 3.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-34000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 136429\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1df1b8fdc8486a989fdc87f4500eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8527 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00036962440935894847, 'eval_runtime': 438.1945, 'eval_samples_per_second': 311.343, 'eval_steps_per_second': 19.459, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-34500\n",
      "Configuration saved in ./results/checkpoint-34500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 3.816113521754428e-06, 'epoch': 4.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-34500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-35000\n",
      "Configuration saved in ./results/checkpoint-35000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0007, 'learning_rate': 3.5815644423595643e-06, 'epoch': 4.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-35000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-35500\n",
      "Configuration saved in ./results/checkpoint-35500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 3.3470153629647006e-06, 'epoch': 4.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-35500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-36000\n",
      "Configuration saved in ./results/checkpoint-36000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 3.1124662835698372e-06, 'epoch': 4.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-36000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-36500\n",
      "Configuration saved in ./results/checkpoint-36500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 2.877917204174974e-06, 'epoch': 4.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-36500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-37000\n",
      "Configuration saved in ./results/checkpoint-37000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 2.64336812478011e-06, 'epoch': 4.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-37000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-37500\n",
      "Configuration saved in ./results/checkpoint-37500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.408819045385247e-06, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-37500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-38000\n",
      "Configuration saved in ./results/checkpoint-38000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 2.174269965990384e-06, 'epoch': 4.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-38000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-38500\n",
      "Configuration saved in ./results/checkpoint-38500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.93972088659552e-06, 'epoch': 4.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-38500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-39000\n",
      "Configuration saved in ./results/checkpoint-39000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 1.7051718072006567e-06, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-39000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-39500\n",
      "Configuration saved in ./results/checkpoint-39500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 1.4706227278057936e-06, 'epoch': 4.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-39500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-40000\n",
      "Configuration saved in ./results/checkpoint-40000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 1.23607364841093e-06, 'epoch': 4.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-40000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-40500\n",
      "Configuration saved in ./results/checkpoint-40500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 1.0015245690160667e-06, 'epoch': 4.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-40500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-41000\n",
      "Configuration saved in ./results/checkpoint-41000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 7.669754896212032e-07, 'epoch': 4.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-41000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-41500\n",
      "Configuration saved in ./results/checkpoint-41500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 5.324264102263399e-07, 'epoch': 4.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-41500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-42000\n",
      "Configuration saved in ./results/checkpoint-42000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.978773308314765e-07, 'epoch': 4.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-42000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-42500\n",
      "Configuration saved in ./results/checkpoint-42500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 6.332825143661312e-08, 'epoch': 4.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-42500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 136429\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbcc2f99c204d9c82359f3d06ca40d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8527 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00019751596846617758, 'eval_runtime': 437.8101, 'eval_samples_per_second': 311.617, 'eval_steps_per_second': 19.476, 'epoch': 5.0}\n",
      "{'train_runtime': 10561.2495, 'train_samples_per_second': 64.589, 'train_steps_per_second': 4.037, 'train_loss': 0.006245762737205974, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42635, training_loss=0.006245762737205974, metrics={'train_runtime': 10561.2495, 'train_samples_per_second': 64.589, 'train_steps_per_second': 4.037, 'train_loss': 0.006245762737205974, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-clinical-scratch-wl-es-NER-prescription\n",
      "Configuration saved in bert-clinical-scratch-wl-es-NER-prescription/config.json\n",
      "Model weights saved in bert-clinical-scratch-wl-es-NER-prescription/pytorch_model.bin\n",
      "tokenizer config file saved in bert-clinical-scratch-wl-es-NER-prescription/tokenizer_config.json\n",
      "Special tokens file saved in bert-clinical-scratch-wl-es-NER-prescription/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"bert-clinical-scratch-wl-es-NER-prescription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auxfunctions import tokenize_and_align_labels\n",
    "\n",
    "MODEL = \"plncmm/bert-clinical-scratch-wl-es\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-clinical-scratch-wl-es-NER-prescription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxfunctions import eval_text, map_entities, calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0,  0,  3,  0,  0,  5,  7,  8,  8,  9, 10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\"\n",
    "\n",
    "eval_text(text,tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['B-ACTIVE_PRINCIPLE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FORMA_FARMA',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ADMIN',\n",
       " 'B-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'B-DURATION',\n",
       " 'I-DURATION',\n",
       " 'I-DURATION']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text)\n",
    "map_entities(eval_text(text,tokenizer,model),ner_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluación con otras métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [row['ner_tags'] for row in HF_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#Y100sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m100\u001b[39m\u001b[39m*\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39mlength \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mprogreso = \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m100\u001b[39m\u001b[39m*\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39mlength))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#Y100sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_preds\u001b[39m.\u001b[39mappend(\u001b[39mlist\u001b[39m(eval_text(row[\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m],tokenizer,model)))\n",
      "File \u001b[0;32m~/entidades-minsal/modelos/auxfunctions.py:109\u001b[0m, in \u001b[0;36meval_text\u001b[0;34m(text, tokenizer, model)\u001b[0m\n\u001b[1;32m    107\u001b[0m mask \u001b[39m=\u001b[39m word_ids_method(text,tokenizer)\n\u001b[1;32m    108\u001b[0m encoded_input \u001b[39m=\u001b[39m tokenizer(text,return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m,is_split_into_words\u001b[39m=\u001b[39m\u001b[39misinstance\u001b[39m(text,\u001b[39mlist\u001b[39m))\n\u001b[0;32m--> 109\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoded_input)\n\u001b[1;32m    110\u001b[0m scores \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    111\u001b[0m scores \u001b[39m=\u001b[39m softmax(scores)\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1751\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1751\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1752\u001b[0m     input_ids,\n\u001b[1;32m   1753\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1754\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1755\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1756\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1757\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1758\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1759\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1760\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1761\u001b[0m )\n\u001b[1;32m   1763\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1765\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1007\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1009\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1015\u001b[0m     embedding_output,\n\u001b[1;32m   1016\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1017\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1018\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1019\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1020\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1021\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1022\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1023\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1024\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1027\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    596\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m    605\u001b[0m         attention_mask,\n\u001b[1;32m    606\u001b[0m         layer_head_mask,\n\u001b[1;32m    607\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m         past_key_value,\n\u001b[1;32m    610\u001b[0m         output_attentions,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    614\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:531\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    528\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    529\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 531\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    534\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    536\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:543\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> 543\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    544\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    545\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:444\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    443\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 444\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/torch/nn/functional.py:1555\u001b[0m, in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m   1554\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(gelu, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1555\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mgelu(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_preds = []\n",
    "length = len(HF_dataset['test'])\n",
    "for i, row in enumerate(HF_dataset['test']):\n",
    "    if 100*(i+1)/length % 10 == 0:\n",
    "        print(\"progreso = {}%\".format(100*(i+1)/length))\n",
    "    y_preds.append(list(eval_text(row['tokens'],tokenizer,model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Métricas en test-etiquetado con ER\")\n",
    "calculate_metrics(y_preds,y_test,ner_dict=ner_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 250 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "datos_conll = Corpus()\n",
    "datos_conll.entidades = ner_dict\n",
    "\n",
    "for i in range(4):\n",
    "    datos_conll.load_conll('../datos/Etiquetado/corpus_s{}_etiquetados.conll'.format(i+1))\n",
    "\n",
    "HF_data_mini = datos_conll.to_HF_dataset()\n",
    "HF_dataset_mini = HF_data_mini.train_test_split(test_size=0.5,seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxfunctions import eval_text, map_entities, calculate_metrics\n",
    "\n",
    "y_test = [row['ner_tags'] for row in HF_dataset_mini['test']]\n",
    "y_preds = [list(eval_text(row['tokens'],tokenizer,model)) for row in HF_dataset_mini['test']]\n",
    "\n",
    "print(\"Métricas en test etiquetado manual\")\n",
    "calculate_metrics(y_preds,y_test,ner_dict=ner_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versión mini\n",
    "\n",
    "A continuación definimos un modelo como el anterior, con la diferencia de que este será entrenado (fine-tunning) en 80% de los datos etiquetados a mano y testeado con 20 % de estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../datos/procesamiento')\n",
    "from corpus import Corpus\n",
    "\n",
    "datos_conll = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTIVE_PRINCIPLE': 1,\n",
    "            'I-ACTIVE_PRINCIPLE': 2,\n",
    "            'B-FORMA_FARMA':3,\n",
    "            'I-FORMA_FARMA':4,\n",
    "            'B-ADMIN': 5,\n",
    "            'I-ADMIN': 6,\n",
    "            'B-PERIODICITY': 7,\n",
    "            'I-PERIODICITY': 8,\n",
    "            'B-DURATION': 9,\n",
    "            'I-DURATION': 10\n",
    "            }\n",
    "\n",
    "datos_conll.entidades = ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregadas 250 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n",
      "Agregadas 251 secuencias de token-entidad al corpus\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    datos_conll.load_conll('../datos/Etiquetado/corpus_s{}_etiquetados.conll'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 501\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 502\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_data_mini = datos_conll.to_HF_dataset()\n",
    "\n",
    "HF_dataset = HF_data_mini.train_test_split(test_size=0.5,seed=0)\n",
    "HF_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489cbb57cfa047249b7a65538382f070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7648dffc7bde469dbbb9b9646aba2636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auxfunctions import tokenize_and_align_labels\n",
    "\n",
    "MODEL = \"plncmm/bert-clinical-scratch-wl-es\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "process = lambda examples: tokenize_and_align_labels(examples,tokenizer)\n",
    "\n",
    "tokenized_data_mini = HF_dataset.map(process, batched=True)\n",
    "tokenized_data_mini = tokenized_data_mini.remove_columns(['id','tokens','ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at plncmm/bert-clinical-scratch-wl-es were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at plncmm/bert-clinical-scratch-wl-es and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model_mini = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=len(ner_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_mini,\n",
    "    args=training_args,\n",
    "    train_dataset = tokenized_data_mini[\"train\"],\n",
    "    eval_dataset = tokenized_data_mini[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 501\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00533157120a47d0a853bb2c813e7621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 502\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809bb386bb5e40d99c484d900d57cce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7695652842521667, 'eval_runtime': 5.6407, 'eval_samples_per_second': 88.996, 'eval_steps_per_second': 5.673, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 502\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0082be496b41a992247406307ca71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5680007338523865, 'eval_runtime': 14.4643, 'eval_samples_per_second': 34.706, 'eval_steps_per_second': 2.212, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 502\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b997e9b09d444de79cdb66a88b148535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4883681833744049, 'eval_runtime': 4.6701, 'eval_samples_per_second': 107.492, 'eval_steps_per_second': 6.852, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 502\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d822211bf63643edad9791772822f349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4615930914878845, 'eval_runtime': 1.3913, 'eval_samples_per_second': 360.802, 'eval_steps_per_second': 22.999, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 502\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bad8c51c1994979a12e7ee5d54937d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44520142674446106, 'eval_runtime': 1.4058, 'eval_samples_per_second': 357.08, 'eval_steps_per_second': 22.762, 'epoch': 5.0}\n",
      "{'train_runtime': 210.5847, 'train_samples_per_second': 11.895, 'train_steps_per_second': 0.76, 'train_loss': 0.6563064575195312, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.6563064575195312, metrics={'train_runtime': 210.5847, 'train_samples_per_second': 11.895, 'train_steps_per_second': 0.76, 'train_loss': 0.6563064575195312, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-clinical-scratch-wl-es-NER-prescription-mini\n",
      "Configuration saved in bert-clinical-scratch-wl-es-NER-prescription-mini/config.json\n",
      "Model weights saved in bert-clinical-scratch-wl-es-NER-prescription-mini/pytorch_model.bin\n",
      "tokenizer config file saved in bert-clinical-scratch-wl-es-NER-prescription-mini/tokenizer_config.json\n",
      "Special tokens file saved in bert-clinical-scratch-wl-es-NER-prescription-mini/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"bert-clinical-scratch-wl-es-NER-prescription-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-clinical-scratch-wl-es-NER-prescription-mini/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-clinical-scratch-wl-es-NER-prescription-mini\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file bert-clinical-scratch-wl-es-NER-prescription-mini/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-clinical-scratch-wl-es-NER-prescription-mini.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_mini = AutoModelForTokenClassification.from_pretrained(\"bert-clinical-scratch-wl-es-NER-prescription-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluación\n",
      "\t f1: 0.62 | precision: 0.53 | recall: 0.74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5306397306397307, 0.7367928938756428, 0.6169504795458994)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auxfunctions import eval_text, map_entities, calculate_metrics\n",
    "\n",
    "y_test = [row['ner_tags'] for row in HF_dataset['test']]\n",
    "y_preds = [list(eval_text(row['tokens'],tokenizer,model_mini)) for row in HF_dataset['test']]\n",
    "\n",
    "calculate_metrics(y_preds,y_test,ner_dict=ner_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('entidades_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60cab8a6eccdbb291cd8546425070b6bbfbad3cae25f59923f3919ec9d23e6db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
