{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de tokens con HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../datos/procesamiento')\n",
    "from corpus import Corpus\n",
    "from etiquetado_entidades import codigo_hfl, split_rule, process_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_db = pd.read_csv('../datos/DATA_HLF_MDS_2.csv',sep=',')\n",
    "\n",
    "code_db = pd.read_excel('../datos/PRINCIPIOS_ACTIVOS_MDS.xlsx')\n",
    "HLF = code_db.loc[:,['PRINCIPIO_ACTIVO','CODIGO_HLF']]\n",
    "HLF_df = codigo_hfl(HLF)\n",
    "\n",
    "df = main_db.join(HLF_df.set_index('CODIGO_MEDICAMENTO'), on='CODIGO_MEDICAMENTO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar principios activos, forma farma y juntar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARA DETECTAR LA POSICION DE ENTIDADES TIPO PRINCIPIO ACTIVO, FORMA FARMACO SE COMPARARAN LOS ELEMENTOS DEL CORPUS CON LAS LISTAS CORRESPONDIENTES.\n",
    "PA = np.unique(split_rule(df['PRINCIPIO_ACTIVO'].dropna().unique()))\n",
    "FF = np.unique(split_rule(df['FORMA_FARMA'].dropna().unique()))\n",
    "\n",
    "rows = (df['PRES_DENOMINACION'] + ' ' + df['RESUMEN']).dropna().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar procesamiento de datos de Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advertencia: error en etiquetado\n",
      "['PREDNISONA', '20', 'MG', 'COMPRIMIDO', '40', 'MG', 'ORAL', 'cada', '24', 'horas', 'durante', '4', 'dias']\n",
      "['B-ACTVPRNCP', 'O', 'O', 'B-ADMIN', 'O', 'O', 'B-ADMIN', 'B-PERIODICITY', 'I-PERIODICITY', 'I-PERIODICITY', 'B-DURATION', 'I-DURATION', 'I-DURATION']\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "\n",
    "for i in range(len(rows)):\n",
    "    row = rows[i].split()\n",
    "    \n",
    "\n",
    "    # begin patch\n",
    "    err_count = 0\n",
    "    try:\n",
    "        tagged_seq = process_column(row,PA,FF)\n",
    "        corpus.append(tagged_seq)\n",
    "    except ValueError:\n",
    "        print(\"Advertencia: error en etiquetado\")\n",
    "        print(tagged_seq.tokens)\n",
    "        print(tagged_seq.tags)\n",
    "        err_count += 1\n",
    "\n",
    "    if i > 100000 + err_count:  # end patch\n",
    "        # if i > 100:\n",
    "        # mismo tamaño que baseline  martin\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cargando el corpus como dataset de HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {'O': 0,\n",
    "            'B-ACTVPRNCP': 1,\n",
    "            'I-ACTVPRNCP': 2,\n",
    "            'B-ADMIN': 3,\n",
    "            'I-ADMIN': 4,\n",
    "            'B-PERIODICITY': 5,\n",
    "            'I-PERIODICITY': 6,\n",
    "            'B-DURATION': 7,\n",
    "            'I-DURATION': 8\n",
    "            }\n",
    "\n",
    "corpus.entidades = ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_dataset = corpus.to_HF_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryTable\n",
       "id: int64\n",
       "tokens: list<item: string>\n",
       "  child 0, item: string\n",
       "ner_tags: list<item: int64>\n",
       "  child 0, item: int64\n",
       "----\n",
       "id: [[0,1,2,3,4,...,99996,99997,99998,99999,100000]]\n",
       "tokens: [[[\"PARACETAMOL\",\"500\",\"MG\",\"COMPRIMIDO\",\"1\",...,\"6\",\"horas\",\"durante\",\"3\",\"dias\"],[\"KETOROLACO\",\"10\",\"MG\",\"COMPRIMIDO\",\"1\",...,\"8\",\"horas\",\"durante\",\"3\",\"dias\"],...,[\"DEXAMETASONA\",\"4\",\"MG/ML\",\"SOLUCIÓN\",\"INYECTABLE\",...,\"UNIDAD\",\"PARENTERAL\",\"cada\",\"8\",\"horas\"],[\"KETOROLACO\",\"TROMETAMOL\",\"30\",\"MG/ML\",\"SOLUCIÓN\",...,\"MG\",\"INTRAVENOSA\",\"EN\",\"BOLO\",\"diaria\"]]]\n",
       "ner_tags: [[[1,0,0,3,0,...,6,6,7,8,8],[1,0,0,3,0,...,6,6,7,8,8],...,[1,0,0,1,2,...,0,0,5,6,6],[1,0,0,0,1,...,0,0,3,0,0]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ACTVPRNCP',\n",
       " 'I-ACTVPRNCP',\n",
       " 'B-ADMIN',\n",
       " 'I-ADMIN',\n",
       " 'B-PERIODICITY',\n",
       " 'I-PERIODICITY',\n",
       " 'B-DURATION',\n",
       " 'I-DURATION']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset.features[f\"ner_tags\"].feature.names = [key for key in ner_dict.keys()]\n",
    "HF_dataset.features[f\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 80000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 20001\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_dataset = HF_dataset.train_test_split(test_size=0.2,seed=0)\n",
    "HF_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos un modelo basado en Transformers de HuggingFace\n",
    "\n",
    "Nuestro modelo será: plncmm/bert-clinical-scratch-wl-es. Ha sido fine-tuneado con texto médico (aunque probablemente no con prescripciones)\n",
    "\n",
    "[Fuente de esta sección](https://huggingface.co/docs/transformers/tasks/token_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"plncmm/bert-clinical-scratch-wl-es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el tokenizador para codificar nuestro input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'aci',\n",
       " '##do',\n",
       " 'tran',\n",
       " '##ex',\n",
       " '##ami',\n",
       " '##co',\n",
       " '1',\n",
       " '##g',\n",
       " '/',\n",
       " '10',\n",
       " '##ml',\n",
       " 'inyec',\n",
       " '##y',\n",
       " '##table',\n",
       " '2',\n",
       " 'unidad',\n",
       " 'intraven',\n",
       " '##osa',\n",
       " 'cada',\n",
       " '2',\n",
       " '##4',\n",
       " 'horas',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = HF_dataset[\"train\"][0]\n",
    "\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos corregir el _mismatch_ entre input tokenizado y la lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ea62a106094c7d9572dd5fdd471131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aebd78169c47a785ed91f5647cfeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = HF_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_data = tokenized_data.remove_columns(['id','tokens','ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': InMemoryTable\n",
       " input_ids: list<item: int32>\n",
       "   child 0, item: int32\n",
       " token_type_ids: list<item: int8>\n",
       "   child 0, item: int8\n",
       " attention_mask: list<item: int8>\n",
       "   child 0, item: int8\n",
       " labels: list<item: int64>\n",
       "   child 0, item: int64\n",
       " ----\n",
       " input_ids: [[[4,15721,1050,1572,2483,...,1748,1129,1003,2596,5],[4,15772,13955,3284,16056,...,1976,1748,997,2596,5],...,[4,1097,23070,14921,3284,...,2427,2242,1074,1707,5],[4,19051,25612,1492,25579,...,1411,2242,1074,1707,5]],[[4,15772,13955,3284,16056,...,15359,1672,1972,12873,5],[4,19769,2470,19624,16057,...,1748,1129,1003,2596,5],...,[4,5536,8413,1213,30962,...,1748,1129,1003,2596,5],[4,6752,5899,30957,4129,...,28982,1976,9934,2940,5]],...,[[4,5647,1699,23565,15709,...,2596,1672,999,12873,5],[4,19890,1577,1343,2633,...,12791,1748,1992,2596,5],...,[4,1067,7785,1197,3092,...,1707,1672,2286,12873,5],[4,6265,1446,1600,30962,...,1748,1129,1003,2596,5]],[[4,5536,8413,1213,30962,...,1976,1748,1413,2885,5],[4,6265,1446,1600,30962,...,1976,1748,997,2596,5],...,[4,6756,13908,2287,1062,...,1096,2907,4179,1497,5],[4,1826,6303,1791,3092,...,15359,1672,2286,12873,5]]]\n",
       " token_type_ids: [[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],...,[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]]\n",
       " attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],...,[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]]]\n",
       " labels: [[[-100,1,-100,2,-100,...,5,6,-100,6,-100],[-100,1,-100,0,0,...,-100,5,6,6,-100],...,[-100,1,-100,-100,0,...,0,0,0,0,-100],[-100,1,-100,-100,2,...,0,0,0,0,-100]],[[-100,1,-100,0,0,...,0,7,8,8,-100],[-100,1,-100,-100,-100,...,5,6,-100,6,-100],...,[-100,1,-100,-100,-100,...,5,6,-100,6,-100],[-100,1,-100,-100,2,...,0,-100,0,0,-100]],...,[[-100,1,-100,2,-100,...,6,7,8,8,-100],[-100,1,-100,-100,-100,...,3,5,6,6,-100],...,[-100,0,-100,-100,0,...,0,7,8,8,-100],[-100,1,-100,-100,-100,...,5,6,-100,6,-100]],[[-100,1,-100,-100,-100,...,-100,5,6,6,-100],[-100,1,-100,-100,-100,...,-100,5,6,6,-100],...,[-100,1,-100,-100,-100,...,0,0,-100,-100,-100],[-100,1,-100,-100,0,...,0,7,8,8,-100]]],\n",
       " 'test': InMemoryTable\n",
       " input_ids: list<item: int32>\n",
       "   child 0, item: int32\n",
       " token_type_ids: list<item: int8>\n",
       "   child 0, item: int8\n",
       " attention_mask: list<item: int8>\n",
       "   child 0, item: int8\n",
       " labels: list<item: int64>\n",
       "   child 0, item: int64\n",
       " ----\n",
       " input_ids: [[[4,1383,13141,1184,1316,...,12873,1672,1306,12873,5],[4,19890,1577,1343,2633,...,1748,1129,1003,2596,5],...,[4,27921,1413,991,4667,...,1129,2594,2049,30996,5],[4,5464,8413,4972,1974,...,1748,1129,1003,2596,5]],[[4,8553,1184,19257,4665,...,2155,1748,1002,2596,5],[4,18956,13908,2287,1062,...,2596,1672,2744,12873,5],...,[4,28672,21335,1575,1673,...,28982,1976,9934,2940,5],[4,14553,30993,8331,5043,...,1411,2242,1074,1707,5]],...,[[4,21335,4769,1141,30362,...,2596,1672,1098,12873,5],[4,8182,23839,8413,1137,...,16056,1751,5631,15359,5],...,[4,19051,25612,1492,25579,...,1020,1748,1001,2596,5],[4,15023,13394,1177,1114,...,12791,1748,997,12873,5]],[[4,6265,1446,1600,30962,...,1976,1748,1129,12873,5]]]\n",
       " token_type_ids: [[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],...,[[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]],[[0,0,0,0,0,...,0,0,0,0,0]]]\n",
       " attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],...,[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1]],[[1,1,1,1,1,...,1,1,1,1,1]]]\n",
       " labels: [[[-100,1,-100,-100,-100,...,6,7,8,8,-100],[-100,1,-100,-100,-100,...,5,6,-100,6,-100],...,[-100,1,0,0,1,...,0,0,0,-100,-100],[-100,1,-100,-100,-100,...,5,6,-100,6,-100]],[[-100,1,-100,-100,0,...,-100,5,6,6,-100],[-100,1,-100,-100,-100,...,6,7,8,8,-100],...,[-100,1,2,-100,0,...,0,-100,0,0,-100],[-100,1,-100,-100,2,...,0,0,0,0,-100]],...,[[-100,1,-100,-100,-100,...,6,7,8,8,-100],[-100,1,-100,-100,-100,...,0,0,0,0,-100],...,[-100,1,-100,-100,2,...,-100,5,6,6,-100],[-100,1,-100,-100,-100,...,4,5,6,6,-100]],[[-100,1,-100,-100,-100,...,-100,5,6,6,-100]]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at plncmm/bert-clinical-scratch-wl-es were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at plncmm/bert-clinical-scratch-wl-es and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL, num_labels=len(ner_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset = tokenized_data[\"train\"],\n",
    "    eval_dataset = tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilo/miniconda3/envs/entidades_env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 80000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8bb9f209594d8290cecf97ad8006e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.164, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.043, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0262, 'learning_rate': 1.88e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0196, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0155, 'learning_rate': 1.8e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0128, 'learning_rate': 1.76e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0112, 'learning_rate': 1.72e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0095, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0095, 'learning_rate': 1.64e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0085, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a23192c0434fad87f3bb07b1f41d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.006247437559068203, 'eval_runtime': 58.9982, 'eval_samples_per_second': 339.01, 'eval_steps_per_second': 21.204, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0072, 'learning_rate': 1.5600000000000003e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0063, 'learning_rate': 1.5200000000000002e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-6500\n",
      "Configuration saved in ./results/checkpoint-6500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0068, 'learning_rate': 1.48e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-7000\n",
      "Configuration saved in ./results/checkpoint-7000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0062, 'learning_rate': 1.4400000000000001e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-7500\n",
      "Configuration saved in ./results/checkpoint-7500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0058, 'learning_rate': 1.4e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-8000\n",
      "Configuration saved in ./results/checkpoint-8000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0051, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-8500\n",
      "Configuration saved in ./results/checkpoint-8500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0053, 'learning_rate': 1.3200000000000002e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-9000\n",
      "Configuration saved in ./results/checkpoint-9000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0044, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-9500\n",
      "Configuration saved in ./results/checkpoint-9500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.004, 'learning_rate': 1.2400000000000002e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-10000\n",
      "Configuration saved in ./results/checkpoint-10000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0049, 'learning_rate': 1.2e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dc7b05089f4620985610e6f0dd0d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0041688005439937115, 'eval_runtime': 59.3497, 'eval_samples_per_second': 337.003, 'eval_steps_per_second': 21.078, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-10500\n",
      "Configuration saved in ./results/checkpoint-10500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.003, 'learning_rate': 1.16e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-11000\n",
      "Configuration saved in ./results/checkpoint-11000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'learning_rate': 1.1200000000000001e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-11500\n",
      "Configuration saved in ./results/checkpoint-11500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'learning_rate': 1.0800000000000002e-05, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-12000\n",
      "Configuration saved in ./results/checkpoint-12000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'learning_rate': 1.04e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-12500\n",
      "Configuration saved in ./results/checkpoint-12500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'learning_rate': 1e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-13000\n",
      "Configuration saved in ./results/checkpoint-13000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0038, 'learning_rate': 9.600000000000001e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-13500\n",
      "Configuration saved in ./results/checkpoint-13500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.003, 'learning_rate': 9.200000000000002e-06, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-14000\n",
      "Configuration saved in ./results/checkpoint-14000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'learning_rate': 8.8e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-14500\n",
      "Configuration saved in ./results/checkpoint-14500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'learning_rate': 8.400000000000001e-06, 'epoch': 2.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-15000\n",
      "Configuration saved in ./results/checkpoint-15000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0029, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8d68b435c64e5bb9c8cbd02616df43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0035770474933087826, 'eval_runtime': 59.2839, 'eval_samples_per_second': 337.377, 'eval_steps_per_second': 21.102, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-15500\n",
      "Configuration saved in ./results/checkpoint-15500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0023, 'learning_rate': 7.600000000000001e-06, 'epoch': 3.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-16000\n",
      "Configuration saved in ./results/checkpoint-16000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-16500\n",
      "Configuration saved in ./results/checkpoint-16500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'learning_rate': 6.800000000000001e-06, 'epoch': 3.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-17000\n",
      "Configuration saved in ./results/checkpoint-17000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0025, 'learning_rate': 6.4000000000000006e-06, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-17500\n",
      "Configuration saved in ./results/checkpoint-17500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0022, 'learning_rate': 6e-06, 'epoch': 3.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-18000\n",
      "Configuration saved in ./results/checkpoint-18000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'learning_rate': 5.600000000000001e-06, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-18500\n",
      "Configuration saved in ./results/checkpoint-18500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 5.2e-06, 'epoch': 3.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-19000\n",
      "Configuration saved in ./results/checkpoint-19000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0022, 'learning_rate': 4.800000000000001e-06, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-19500\n",
      "Configuration saved in ./results/checkpoint-19500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0022, 'learning_rate': 4.4e-06, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-20000\n",
      "Configuration saved in ./results/checkpoint-20000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b734edd1cfae40878d6b6719e0aabacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.003310434753075242, 'eval_runtime': 59.0835, 'eval_samples_per_second': 338.521, 'eval_steps_per_second': 21.173, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-20500\n",
      "Configuration saved in ./results/checkpoint-20500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'learning_rate': 3.6000000000000003e-06, 'epoch': 4.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-21000\n",
      "Configuration saved in ./results/checkpoint-21000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 3.2000000000000003e-06, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-21500\n",
      "Configuration saved in ./results/checkpoint-21500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0017, 'learning_rate': 2.8000000000000003e-06, 'epoch': 4.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-22000\n",
      "Configuration saved in ./results/checkpoint-22000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-22500\n",
      "Configuration saved in ./results/checkpoint-22500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-23000\n",
      "Configuration saved in ./results/checkpoint-23000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-23500\n",
      "Configuration saved in ./results/checkpoint-23500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 1.2000000000000002e-06, 'epoch': 4.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-24000\n",
      "Configuration saved in ./results/checkpoint-24000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-24500\n",
      "Configuration saved in ./results/checkpoint-24500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 4.0000000000000003e-07, 'epoch': 4.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-25000\n",
      "Configuration saved in ./results/checkpoint-25000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-25000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20001\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0184c8a7dd548bb81e49b0aa44f85ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.003244617022573948, 'eval_runtime': 58.6449, 'eval_samples_per_second': 341.053, 'eval_steps_per_second': 21.332, 'epoch': 5.0}\n",
      "{'train_runtime': 4891.2761, 'train_samples_per_second': 81.778, 'train_steps_per_second': 5.111, 'train_loss': 0.008898326160907745, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25000, training_loss=0.008898326160907745, metrics={'train_runtime': 4891.2761, 'train_samples_per_second': 81.778, 'train_steps_per_second': 5.111, 'train_loss': 0.008898326160907745, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert-clinical-scratch-wl-es-NER-prescription\n",
      "Configuration saved in bert-clinical-scratch-wl-es-NER-prescription/config.json\n",
      "Model weights saved in bert-clinical-scratch-wl-es-NER-prescription/pytorch_model.bin\n",
      "tokenizer config file saved in bert-clinical-scratch-wl-es-NER-prescription/tokenizer_config.json\n",
      "Special tokens file saved in bert-clinical-scratch-wl-es-NER-prescription/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"bert-clinical-scratch-wl-es-NER-prescription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-clinical-scratch-wl-es-NER-prescription/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-clinical-scratch-wl-es-NER-prescription\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file bert-clinical-scratch-wl-es-NER-prescription/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at bert-clinical-scratch-wl-es-NER-prescription.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-clinical-scratch-wl-es-NER-prescription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "def eval_text(text):\n",
    "    encoded_input = tokenizer(text,return_tensors='pt',is_split_into_words=isinstance(text,list))\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return np.argmax(scores,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"PARACETAMOL 500 MG COMPRIMIDO 1 COMPRIMIDO ORAL cada 6 horas durante 3 dias\"\n",
    "\n",
    "eval_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_entities(y_pred,map_dict):\n",
    "    inv_map = {v: k for k, v in map_dict.items()}\n",
    "    return np.array([inv_map[y] for y in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I-PERIODICITY', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O'], dtype='<U13')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_entities(eval_text(text),ner_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [row['ner_tags'] for row in HF_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = [list(eval_text(row['tokens'])) for row in HF_dataset['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def calculate_metrics(y_pred, y_true, print=True):\n",
    "    \"\"\"\n",
    "    Calcula precision, recall y f1\n",
    "    \"\"\"\n",
    "    # calcular scores\n",
    "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
    "    precision = precision_score(y_true, y_pred, mode='strict')\n",
    "    recall = recall_score(y_true, y_pred, mode='strict')\n",
    "\n",
    "    if print:\n",
    "        print(\"Resultados de evaluación\")\n",
    "        print(f'\\t f1: {f1:.2f} | precision: {precision:.2f} | recall: {recall:.2f}')\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples:\n[14, 12, 15, 13, 15, 16, 11, 17, 17, 15, 12, 10, 10, 17, 14, 15, 8, 14, 16, 24, 15]\n[24, 17, 29, 19, 27, 28, 15, 32, 28, 27, 19, 14, 16, 31, 19, 25, 13, 25, 24, 47, 30]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m calculate_metrics(y_preds,y_test)\n",
      "\u001b[1;32m/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb Cell 41\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(y_pred, y_true, print)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mCalcula precision, recall y f1\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# calcular scores\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m f1 \u001b[39m=\u001b[39m f1_score(y_true, y_pred, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m precision \u001b[39m=\u001b[39m precision_score(y_true, y_pred, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/entidades-minsal/modelos/token_clf_HF.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m recall \u001b[39m=\u001b[39m recall_score(y_true, y_pred, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:359\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, average, suffix, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    350\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support_v1(y_true, y_pred,\n\u001b[1;32m    351\u001b[0m                                                     average\u001b[39m=\u001b[39maverage,\n\u001b[1;32m    352\u001b[0m                                                     warn_for\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mf-score\u001b[39m\u001b[39m'\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    356\u001b[0m                                                     scheme\u001b[39m=\u001b[39mscheme,\n\u001b[1;32m    357\u001b[0m                                                     suffix\u001b[39m=\u001b[39msuffix)\n\u001b[1;32m    358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     _, _, f, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(y_true, y_pred,\n\u001b[1;32m    360\u001b[0m                                                  average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m    361\u001b[0m                                                  warn_for\u001b[39m=\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mf-score\u001b[39;49m\u001b[39m'\u001b[39;49m,),\n\u001b[1;32m    362\u001b[0m                                                  beta\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    363\u001b[0m                                                  sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    364\u001b[0m                                                  zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m    365\u001b[0m                                                  suffix\u001b[39m=\u001b[39;49msuffix)\n\u001b[1;32m    366\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:130\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, suffix)\u001b[0m\n\u001b[1;32m    126\u001b[0m         true_sum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(true_sum, \u001b[39mlen\u001b[39m(entities_true_type))\n\u001b[1;32m    128\u001b[0m     \u001b[39mreturn\u001b[39;00m pred_sum, tp_sum, true_sum\n\u001b[0;32m--> 130\u001b[0m precision, recall, f_score, true_sum \u001b[39m=\u001b[39m _precision_recall_fscore_support(\n\u001b[1;32m    131\u001b[0m     y_true, y_pred,\n\u001b[1;32m    132\u001b[0m     average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m    133\u001b[0m     warn_for\u001b[39m=\u001b[39;49mwarn_for,\n\u001b[1;32m    134\u001b[0m     beta\u001b[39m=\u001b[39;49mbeta,\n\u001b[1;32m    135\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    136\u001b[0m     zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m    137\u001b[0m     scheme\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    138\u001b[0m     suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m    139\u001b[0m     extract_tp_actual_correct\u001b[39m=\u001b[39;49mextract_tp_actual_correct\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m precision, recall, f_score, true_sum\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/seqeval/metrics/v1.py:122\u001b[0m, in \u001b[0;36m_precision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, scheme, suffix, extract_tp_actual_correct)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options:\n\u001b[1;32m    120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(average_options))\n\u001b[0;32m--> 122\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    124\u001b[0m pred_sum, tp_sum, true_sum \u001b[39m=\u001b[39m extract_tp_actual_correct(y_true, y_pred, suffix, scheme)\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/entidades_env/lib/python3.9/site-packages/seqeval/metrics/v1.py:101\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_true) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(y_pred) \u001b[39mor\u001b[39;00m len_true \u001b[39m!=\u001b[39m len_pred:\n\u001b[1;32m    100\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(len_true, len_pred)\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples:\n[14, 12, 15, 13, 15, 16, 11, 17, 17, 15, 12, 10, 10, 17, 14, 15, 8, 14, 16, 24, 15]\n[24, 17, 29, 19, 27, 28, 15, 32, 28, 27, 19, 14, 16, 31, 19, 25, 13, 25, 24, 47, 30]"
     ]
    }
   ],
   "source": [
    "calculate_metrics(y_preds,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('entidades_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60cab8a6eccdbb291cd8546425070b6bbfbad3cae25f59923f3919ec9d23e6db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
